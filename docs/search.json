[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 303",
    "section": "",
    "text": "This course was originally developed in collaboration with Sarah Teichman."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This syllabus is subject to change.\n\nCourse Information\nInstructor: Peter Gao (petergao at uw dot edu)\nTime: Tuesdays and Thursdays, 4:00 - 5:20pm\nLocation: GUG 204\nOffice hours: TBD\nNote: When emailing the instructor, please include “[STAT 303]” at the beginning of the email header! It will help us respond to you faster.\n\n\nOverview\nUsing examples from medicine, education, and criminal justice, this course surveys ethical & social implications of the design, implementation, & interpretation of statistical decision-making algorithms. Students will examine how algorithms interact with social categories including race, class, & gender, preserving or reshaping existing inequities. Students will evaluate statistical frameworks for balancing fairness & privacy with efficiency.\nCourse objectives\n\nIdentify and discuss ethical considerations related to the design, implementation, and results of decision making algorithms.\nExamine historical uses of data in decision making and draw connections to modern algorithmic decision making systems in fields including medicine, education, and criminal justice.\nUnderstand and critique statistical approaches to understanding fairness and privacy, especially as they pertain to data-driven decision making.\nPractice communicating about ethical considerations in data science through writing, discussion, and presentation.\n\n\n\nOutline\nIn this course, we will cover the following:\n\nIntroduction to ethics and algorithms\nAlgorithmic fairness and bias\nFacial recognition technologies\nAlgorithms in the public and private spheres\nPrivacy and surveillance\nPaths forward\nFinal Presentations\n\n\n\nCourse structure\n\nIn-class Discussion (10%): You will be expected to attend class regularly and to participate in class activities and discussions. Please try to bring something to each class: a question, an idea, a connection, etc. In addition, you will serve as a designated discussant for two class sessions during the quarter. As a discussant, you will prepare discussion questions in response to readings and be called upon to respond to questions that arise during class. More detailed instructions will be provided at the start of the quarter.\nIn-class Work (10%): In order to give students the flexibility to miss class as personal/health matters arise, attendance will not contribute to final grades. Instead, students will be asked to complete brief in-class assignments, which students may submit in person or online if they are unable to attend. You will receive zero, half, or full credit based upon completeness and depth of your responses.\nLong Responses (45%): Throughout the quarter, students will complete three writing assignments. These assignments ask students to write brief memos that could be used to communicate about issues related to algorithmic decision systems to a general public. These responses will be graded according to completeness and depth of your work.\n\nResponse 1 asks students to study an algorithm and identify potential ethical concerns.\nResponse 2 concerns statistical frameworks for assessing algorithmic bias and fairness.\nResponse 3 concerns forms of resistance to algorithmic decision systems.\n\nFinal Project (35%): In groups or by yourself, you will select a topic from class and extend our discussion (ex. By further examining the human impacts of a given algorithm). Halfway through the course, you will submit a final project proposal. During the 10th week, you will submit a written report and self assessment and present a brief talk (< 10 minutes) in-class.\n\nObtaining a good grade in this class will depend on your ability to stay organized and complete readings and assignments on time, but if you attend class with the goal of helping us to build a healthy discussion, you can and should be successful.\nIn general, the late policy is as follows: Any assignment that is received late but less than 24 hours late will receive a grade penalty of 25%. Any assignment that is received 24–48 hours late will receive a grade penalty of 50%. Assignments will not be accepted more than 48 hours late. That said, if you communicate directly with me before an assignment is due, I will often be willing to relax a deadline.\n\nExpectations\nThe COVID-19 pandemic has and will continue to present many of us with unforeseen difficulties. I encourage all of you to prioritize the health and safety of yourselves and those around you and would be happy to make accommodations that help you to do so. In addition, I hope that we can be patient with one another as we begin transitioning back to in-person learning, as it is likely that not everything will go to plan. Please feel free to reach out to me via email at any point to discuss any concerns you may have about the course.\n\n\nKeeping each other safe\nI am thrilled to be teaching in person and I hope you are excited to be back on campus as well. As we return to physical classrooms, please be respectful of your classmates’ boundaries and precautions–we are all readjusting to in-person learning. In addition, I hope we will all make every effort to keep ourselves and our classmates safe. If you test positive or are exposed to possible infection, I encourage you to err on the side of caution with regards to attending classes and would be happy to make accommodations that allow you to do so. I will make high-quality recordings of lecture and additional office hours available to students that are absent due to quarantine.\n\n\n\nClassroom environment\n\nNames & Pronouns\nEveryone deserves to be addressed as they would like. Feel free to send us your preferred name and correct pronouns at any time.\n\n\nFeedback\nI encourage and appreciate your feedback throughout the quarter. You are welcome to provide feedback on any aspect of the course at any time via email or in person. If you would prefer to do so confidentially, you can do so through the form here.\n\n\nParticipation expectations\nAbove all, we are trying to build a space for spirited discussion and learning. Come to class prepared to share what you have learned and to ask questions of us and your classmates. To be more specific:\n\nRead and review the assigned texts before class. Every class, try to bring something to contribute. What kind of something? Bring a question to ask the class, an idea to share, or a connection you drew to current events or other courses you have taken. If you loved a reading–great! We want to hear about it. If you hated a reading or think that a writer has it all wrong, also great!\nWe hope that you will feel comfortable taking risks in discussion. You do not always have to have the “right” answers or to prepare exactly what you want to say before you raise your hand. We are here to figure it out together, but you must take the first step by beginning to speak.\nListen to your classmates and really consider what they have to say. We are all here to learn–challenge each other by asking questions or building on each other’s ideas. Along these lines: don’t hog the mic! Give each other time to think and speak. It is our collective responsibility to help each other feel comfortable sharing our thoughts and ideas in the class.\nYou may sometimes feel that you do not have anything to contribute to a discussion. This is okay! We trust your judgment: sometimes the best way to contribute is by listening and taking some time to process your own thoughts.\n\n\n\nAcademic misconduct\nAcademic integrity is essential to this course and to your learning. On certain assignments, collaboration is allowed and encouraged when following the collaboration policy outlined above. Violations of the academic integrity policy include but are not limited to: copying from a peer, collaborating where it is not allowed, copying from an online resource, using a solutions manual, and using resources from a previous iteration of the course. Anything found in violation of this policy will be automatically given a score of 0 with no exceptions. If the situation merits, it will also be reported to the UW Student Conduct Office, at which point it will be out of my hands. If you have any questions about this policy, please do not hesitate to reach out and ask.\nThe university’s policy on plagiarism and academic misconduct is a part of the Student Conduct Code, which cites the definition of academic misconduct in the WAC 478-121. (WAC is an abbreviation for the Washington Administrative Code, the set of state regulations for the university. The entire chapter of the WAC on the student conduct code is here http://www.washington.edu/admin/rules/policies/WAC/478-121TOC.html) According to this section of the WAC, academic misconduct includes:\n“Cheating”—such as “unauthorized assistance in taking quizzes”, “Falsification” “which is the intentional use or submission of falsified data, records, or other information including, but not limited to, records of internship or practicum experiences or attendance at any required event(s), or scholarly research”; and “Plagiarism” which includes “[t]he use, by paraphrase or direct quotation, of the published or unpublished work of another person without full and clear acknowledgment.”\nThe UW Libraries have a useful guide for students at http://www.lib.washington.edu/teaching/plagiarism Students found to have engaged in academic misconduct may receive a zero on the assignment (or other possible outcome).\n\n\nConduct\nThe University of Washington Student Conduct Code (WAC 478-121) defines prohibited academic and behavioral conduct and describes how the University holds students accountable as they pursue their academic goals. Allegations of misconduct by students may be referred to the appropriate campus office for investigation and resolution. More information can be found online at https://www.washington.edu/studentconduct/.\n\n\nDisability Resources\nYour experience in this class is important to me. It is the policy and practice of the University of Washington to create inclusive and accessible learning environments consistent with federal and state law. If you have already established accommodations with Disability Resources for Students (DRS), please activate your accommodations via myDRS so we can discuss how they will be implemented in this course.\nIf you have not yet established services through DRS, but have a temporary health condition or permanent disability that requires accommodations (conditions include but not limited to; mental health, attention-related, learning, vision, hearing, physical or health impacts), contact DRS directly to set up an Access Plan. DRS facilitates the interactive process that establishes reasonable accommodations. Contact DRS at http://depts.washington.edu/uwdrs/\n\n\nDiversity, equity and inclusion\nDiverse backgrounds, embodiments, and experiences are essential to the critical thinking endeavor at the heart of university education. Therefore, I expect you to follow the UW Student Conduct Code in your interactions with your colleagues and me in this course by respecting the many social and cultural differences among us, which may include, but are not limited to: age, cultural background, disability, ethnicity, family status, gender identity and presentation, citizenship and immigration status, national origin, race, religious and political beliefs, sex, sexual orientation, socioeconomic status, and veteran status.\n\n\nReligious accommodations\nWashington state law requires that UW develop a policy for accommodation of student absences or significant hardship due to reasons of faith or conscience, or for organized religious activities. The UW’s policy, including more information about how to request an accommodation, is available at Religious Accommodations Policy (https://registrar.washington.edu/staffandfaculty/religious-accommodations-policy/). Accommodations must be requested within the first two weeks of this course using the Religious Accommodations Request form (https://registrar.washington.edu/students/religious-accommodations-request/).\n\n\nStudent privacy\nNote that the software used in this class (e.g. Canvas, Zoom, Panopto) when used with our UW Net IDs, are FERPA compliant (https://registrar.washington.edu/students/ferpa/). This means they do not monitor student use of their service and they do not share student data with third parties.\nSharing recordings and other class materials outside of class that include personally identifiable student information without the written consent of those students is a violation of FERPA. State law requires consent from people to be recorded (https://apps.leg.wa.gov/rcw/default.aspx?cite=9.73.030), please note that (1) that your participation in this class indicates your consent for course activities to be recorded, (2) you are not permitted to make your own recordings without consent from the instructor and everyone else involved, and (3) that the instructor’s recordings will be available for later playback only to students taking the course. For more information about privacy concerns, review the UW Privacy Office policies (https://privacy.uw.edu/policies/best-practices-online-conferencing/), or contact Helen Garrett, the UW’s FERPA Officer.\n\n\n\nLinks and other resources\n\nCanvas Page"
  },
  {
    "objectID": "readings.html",
    "href": "readings.html",
    "title": "Readings",
    "section": "",
    "text": "Date\nReadings\nDue\n\n\n\n\n9/29\nAllen Institute for AI. Ask Delphi. 2021, https://delphi.allenai.org/.\nBonde, Sheila, and Paul Firenze. A Framework for Making Ethical Decisions. May 2013, https://www.brown.edu/academics/science-and-technology-studies/framework-making-ethical-decisions.\nGupta, Abhishek, and Victoria Heath. “AI Ethics Groups Are Repeating One of Society’s Classic Mistakes.” MIT Technology Review, Sept. 2020, https://www.technologyreview.com/2020/09/14/1008323/ai-ethics-representation-artificial-intelligence-opinion/.\nJiang, Liwei. “Towards Machine Ethics and Norms.” AI2 Blog, 4 Nov. 2021, https://medium.com/ai2-blog/towards-machine-ethics-and-norms-d64f2bdde6a3.\nRaji, Deborah. “That’s Not Fair!” XRDS: Crossroads, The ACM Magazine for Students, vol. 25, no. 3, Apr. 2019, pp. 44–48. Spring 2019, https://doi.org/10.1145/3313127.\n\n\n\n10/4\nACLU of Washington. “Automated Decision Making Systems Are Making Some of the Most Important Life Decisions For You, but You Might Not Even Know It.” ACLU of Washington, 22 Sept. 2021, https://www.aclu-wa.org/story/automated-decision-making-systems-are-making-some-most-important-life-decisions-you-you-might.\nKearns, Michael, and Aaron Roth. “Introduction.” The Ethical Algorithm: The Science of Socially Aware Algorithm Design, Oxford University Press USA - OSO, 2019. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/washington/detail.action?docID=5905172.\nLum, Kristian, and Rumman Chowdhury. “What Is an ‘Algorithm’? It Depends Whom You Ask.” MIT Technology Review, Feb. 2021, https://www.technologyreview.com/2021/02/26/1020007/what-is-an-algorithm/.\n\n\n\n10/6\nBembeneck, Emily, et al. “To Stop Algorithmic Bias, We First Have to Define It.” Brookings, 21 Oct. 2021, https://www.brookings.edu/research/to-stop-algorithmic-bias-we-first-have-to-define-it/.\nBenjamin, Ruha. “Default Discrimination.” Race after Technology: Abolitionist Tools for the New Jim Code, Polity Press, 2019. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/washington/detail.action?docID=5820427.\nChowdhury, Rumman. Sharing Learnings about Our Image Cropping Algorithm. 19 May 2021, https://blog.twitter.com/engineering/en_us/topics/insights/2021/sharing-learnings-about-our-image-cropping-algorithm.\nFriedman, Batya, and Helen Nissenbaum. “Bias in Computer Systems.” ACM Transactions on Information Systems, vol. 14, no. 3, July 1996, pp. 330–47. July 1996, https://doi.org/10.1145/230538.230561.\nVox. Are We Automating Racism? 2021. YouTube, https://www.youtube.com/watch?v=Ok5sKLXqynQ.\n\n\n\n10/11\nFeathers, Todd. Police Are Telling ShotSpotter to Alter Evidence From Gunshot-Detecting AI. 26 July 2021, https://www.vice.com/en/article/qj8xbq/police-are-telling-shotspotter-to-alter-evidence-from-gunshot-detecting-ai.\nMitchell, Shira, et al. “Algorithmic Fairness: Choices, Assumptions, and Definitions.” Annual Review of Statistics and Its Application, vol. 8, no. 1, 2021, p. null. Annual Reviews, https://doi.org/10.1146/annurev-statistics-042720-125902.\nSmith, Ben. “How TikTok Reads Your Mind.” The New York Times, 6 Dec. 2021. NYTimes.com, https://www.nytimes.com/2021/12/05/business/media/tiktok-algorithm.html.\n\n\n\n10/13\nJeffries, Adrianne, et al. “Swinging the Vote?” The Markup, Feb. 2020, https://themarkup.org/google-the-giant/2020/02/26/wheres-my-email.\nKirchner, Lauren. “Powerful DNA Software Used in Hundreds of Criminal Cases Faces New Scrutiny.” The Markup, Mar. 2021, https://themarkup.org/news/2021/03/09/powerful-dna-software-used-in-hundreds-of-criminal-cases-faces-new-scrutiny.\n---. “When Zombie Data Costs You a Home.” The Markup, Oct. 2020, https://themarkup.org/locked-out/2020/10/06/zombie-criminal-records-housing-background-checks.\nVarner, Maddy, and Aaron Sankin. “Suckers List: How Allstate’s Secret Auto Insurance Algorithm Squeezes Big Spenders.” The Markup, Feb. 2020, https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list.\n\n\n\n10/18\nFry, Hannah. “What Statistics Can and Can’t Tell Us About Ourselves.” The New Yorker, Sept. 2019, https://www.newyorker.com/magazine/2019/09/09/what-statistics-can-and-cant-tell-us-about-ourselves.\nOchigame, Rodrigo. “The Long History of Algorithmic Fairness.” Phenomenal World, 30 Jan. 2020, https://phenomenalworld.org/analysis/long-history-algorithmic-fairness.\nOnuoha, Mimi. “When Proof Is Not Enough.” FiveThirtyEight, 1 July 2020, https://fivethirtyeight.com/features/when-proof-is-not-enough/.\nWhitby, Andrew. “A Brief History of the Census—and How Covid-19 Could Change It.” Wired, Apr. 2020. www.wired.com, https://www.wired.com/story/brief-history-census-how-covid-19-could-change-it/.\n\n\n\n10/20\nBuolamwini, Joy, et al. Facial Recognition Technologies: A Primer. 29 May 2020, https://global-uploads.webflow.com/5e027ca188c99e3515b404b7/5ed1002058516c11edc66a14_FRTsPrimerMay2020.pdf.\nHill, Kashmir, and Aaron Krolik. “How Photos of Your Kids Are Powering Surveillance Technology.” The New York Times, 11 Oct. 2019. NYTimes.com, https://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-recognition.html.\nMcVean, Ada. “40 Years of Human Experimentation in America: The Tuskegee Study.” Office for Science and Society, 25 Jan. 2019, https://www.mcgill.ca/oss/article/history/40-years-human-experimentation-america-tuskegee-study.\n\n\n\n10/25\nBuolamwini, Joy, and Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” Conference on Fairness, Accountability and Transparency, PMLR, 2018, pp. 77–91. proceedings.mlr.press, http://proceedings.mlr.press/v81/buolamwini18a.html.\nCastelvecchi, Davide. “Is Facial Recognition Too Biased to Be Let Loose?” Nature, vol. 587, no. 7834, 7834, Nature Publishing Group, Nov. 2020, pp. 347–49. www.nature.com, https://doi.org/10.1038/d41586-020-03186-4.\nMIT Media Lab. Gender Shades. 2018. YouTube, https://www.youtube.com/watch?v=TWWsW1w-BVo.\nNoorden, Richard Van. “The Ethical Questions That Haunt Facial-Recognition Research.” Nature, vol. 587, no. 7834, 7834, Nature Publishing Group, Nov. 2020, pp. 354–58. www.nature.com, https://doi.org/10.1038/d41586-020-03187-3.\n\n\n\n10/27\nAngwin, Julia, et al. “Machine Bias.” ProPublica, 23 May 2016, https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing?token=jRdnwabwdw5HLiHY-R3nqWS5DOjEM7W-.\nHill, Kashmir. “Wrongfully Accused by an Algorithm.” The New York Times, 24 June 2020. NYTimes.com, https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html.\nLum, Kristian, and William Isaac. “To Predict and Serve?” Significance, vol. 13, no. 5, 2016, pp. 14–19. Wiley Online Library, https://doi.org/10.1111/j.1740-9713.2016.00960.x.\nO’Neill, James. “How Facial Recognition Makes You Safer.” The New York Times, 9 June 2019. NYTimes.com, https://www.nytimes.com/2019/06/09/opinion/facial-recognition-police-new-york-city.html.\n\n\n\n11/1\nArvind Narayanan. Tutorial: 21 Fairness Definitions and Their Politics. 2018. YouTube, https://www.youtube.com/watch?v=jIXIuYdnyyk.\nChouldechova, Alexandra. “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.” Big Data, vol. 5, no. 2, Mary Ann Liebert, Inc., publishers, June 2017, pp. 153–63. liebertpub.com (Atypon), https://doi.org/10.1089/big.2016.0047.\nMitchell, Shira, et al. “Algorithmic Fairness: Choices, Assumptions, and Definitions.” Annual Review of Statistics and Its Application, vol. 8, no. 1, 2021, p. null. Annual Reviews, https://doi.org/10.1146/annurev-statistics-042720-125902.\n\n\n\n11/3\nCrawford, Kate. “State.” The Atlas of AI, Yale University Press, 2021, pp. 181–209. JSTOR, https://doi.org/10.2307/j.ctv1ghv45t.9.\nGreen, Ben. The Innovative City: The Relationship between Technical and Nontechnical Change in City Government. 2019. direct.mit.edu, https://direct.mit.edu/books/book/4204/chapter/172388/The-Innovative-City-The-Relationship-between.\n\n\n\n11/8\nIngold, David, and Spencer Soper. “Amazon Doesn’t Consider the Race of Its Customers. Should It?” Bloomberg.Com, 21 Apr. 2016, http://www.bloomberg.com/graphics/2016-amazon-same-day/.\nMIT Technology Review. Podcast: In Machines We Trust - Hired by an Algorithm. 2021. YouTube, https://www.youtube.com/watch?v=ztcVB_zh_M0.\nRaghavan, Manish, and Solon Barocas. “Challenges for Mitigating Bias in Algorithmic Hiring.” Brookings, 6 Dec. 2019, https://www.brookings.edu/research/challenges-for-mitigating-bias-in-algorithmic-hiring/.\n\n\n\n11/10\nBooth, Robert. “Uber Drivers to Launch Legal Bid to Uncover App’s Algorithm.” The Guardian, 20 July 2020, http://www.theguardian.com/technology/2020/jul/20/uber-drivers-to-launch-legal-bid-to-uncover-apps-algorithm.\nCrawford, Kate. “Labor.” The Atlas of AI, Yale University Press, 2021, pp. 53–87. JSTOR, https://doi.org/10.2307/j.ctv1ghv45t.5.\nGurley, Lauren Kaori. “Amazon Drivers Are Instructed to Drive Recklessly to Meet Delivery Quotas.” VICE, 6 May 2021, https://www.vice.com/en/article/xgxx54/amazon-drivers-are-instructed-to-drive-recklessly-to-meet-delivery-quotas.\nThe Financial Times. The Uber Game. 2020, https://ig.ft.com/uber-game.\n\n\n\n11/15\nEubanks, Virginia. “A Child Abuse Prediction Model Fails Poor Families.” Wired, Jan. 2018. www.wired.com, https://www.wired.com/story/excerpt-from-automating-inequality/.\nGilman, Michele. Poverty Lawgorithms. Data & Society, 15 Sept. 2020, https://datasociety.net/library/poverty-lawgorithms/.\nHenriques-Gomes, Luke. “The Automated System Leaving Welfare Recipients Cut off with Nowhere to Turn.” The Guardian, 16 Oct. 2019, http://www.theguardian.com/technology/2019/oct/16/automated-messages-welfare-australia-system.\nSudhir, K., and Shyam Sunder. “What Happens When a Billion Identities Are Digitized?” Yale Insights, 27 Mar. 2020, https://insights.som.yale.edu/insights/what-happens-when-billion-identities-are-digitized.\n\n\n\n11/17\nChen, Irene Y., et al. “Ethical Machine Learning in Healthcare.” Annual Review of Biomedical Data Science, vol. 4, no. 1, 2021, p. null. Annual Reviews, https://doi.org/10.1146/annurev-biodatasci-092820-114757.\nObermeyer, Ziad, et al. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science, vol. 366, no. 6464, American Association for the Advancement of Science, Oct. 2019, pp. 447–53. www-science-org.offcampus.lib.washington.edu (Atypon), https://doi.org/10.1126/science.aax2342.\n\n\n\n11/22\nJeffrey Mervis. “Can a Set of Equations Keep U.S. Census Data Private?” ScienceInsider, 4 Jan. 2019, https://www.science.org/content/article/can-set-equations-keep-us-census-data-private.\nWezerek, Gus, and David Van Riper. “Opinion | Changes to the Census Could Make Small Towns Disappear.” The New York Times, 6 Feb. 2020. NYTimes.com, https://www.nytimes.com/interactive/2020/02/06/opinion/census-algorithm-privacy.html.\nWolford, Ben. “What Is GDPR, the EU’s New Data Protection Law?” GDPR.Eu, 7 Nov. 2018, https://gdpr.eu/what-is-gdpr/.\nWood, Alexandra, et al. “Differential Privacy: A Primer for a Non-Technical Audience.” Vanderbilt Journal of Entertainment & Technology Law, vol. 21, no. 1, 2018, pp. 209–75.\n\n\n\n11/24\nNO CLASS\n\n\n\n11/29\nGreen, Ben. “The False Promise of Risk Assessments: Epistemic Reform and the Limits of Fairness.” Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, Association for Computing Machinery, 2020, pp. 594–606. ACM Digital Library, https://doi.org/10.1145/3351095.3372869.\nHooker, Sara. “Moving beyond ‘Algorithmic Bias Is a Data Problem.’” Patterns, vol. 2, no. 4, Elsevier, Apr. 2021. www.cell.com, https://doi.org/10.1016/j.patter.2021.100241.\nRaji, Inioluwa Deborah, and Joy Buolamwini. “Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products.” Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, Association for Computing Machinery, 2019, pp. 429–35. ACM Digital Library, https://doi.org/10.1145/3306618.3314244.\nUpchurch, Tom. “To Work for Society, Data Scientists Need a Hippocratic Oath with Teeth.” Wired UK. www.wired.co.uk, https://www.wired.co.uk/article/data-ai-ethics-hippocratic-oath-cathy-o-neil-weapons-of-math-destruction. Accessed 10 May 2021.\n\n\n\n12/1\nCampbell, Alexia Fernández. “How Tech Employees Are Pushing Silicon Valley to Put Ethics before Profit.” Vox, Oct. 2018, https://www.vox.com/technology/2018/10/18/17989482/google-amazon-employee-ethics-contracts.\nHeilweil, Rebecca. “Facebook Is Taking a Hard Look at Racial Bias in Its Algorithms.” Vox, July 2020, https://www.vox.com/recode/2020/7/22/21334051/facebook-news-feed-instagram-algorithm-racial-bias-civil-rights-audit.\nMullainathan, Sendhil. “Biased Algorithms Are Easier to Fix Than Biased People.” The New York Times, 6 Dec. 2019. NYTimes.com, https://www.nytimes.com/2019/12/06/business/algorithm-bias-fix.html.\nNIckelsburg, Monica. “Washington State Lawmakers Seek to Ban Government from Using Discriminatory AI Tech.” GeekWire, Feb. 2021, https://www.geekwire.com/2021/washington-state-lawmakers-seek-ban-government-using-ai-tech-discriminates/.\nRichardson, Rashida. Confronting Black Boxes: A Shadow Report of the New York City Automated Decision System Task Force. AI Now Institute, 4 Dec. 2019, https:// ainowinstitute.org/ads-shadowreport-2019.html.\n\n\n\n12/6\n\nFinal Presentations\n\n\n12/8\n\nFinal Presentations"
  },
  {
    "objectID": "readings_alt.html",
    "href": "readings_alt.html",
    "title": "Readings",
    "section": "",
    "text": "References\n\nACLU of Washington. 2021. “Automated Decision Making Systems Are Making Some of the Most Important Life Decisions For You, but You Might Not Even Know It.” https://www.aclu-wa.org/story/automated-decision-making-systems-are-making-some-most-important-life-decisions-you-you-might.\n\n\nAllen Institute for AI. 2021. “Ask Delphi.” https://delphi.allenai.org/.\n\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. “Machine Bias.” https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing?token=jRdnwabwdw5HLiHY-R3nqWS5DOjEM7W-.\n\n\nArvind Narayanan. 2018. “Tutorial: 21 Fairness Definitions and Their Politics,” March. https://www.youtube.com/watch?v=jIXIuYdnyyk.\n\n\nBembeneck, Emily, Rebecca Nissan, and Ziad Obermeyer. 2021. “To Stop Algorithmic Bias, We First Have to Define It.” https://www.brookings.edu/research/to-stop-algorithmic-bias-we-first-have-to-define-it/.\n\n\nBenjamin, Ruha. 2019. “Default Discrimination.” In Race After Technology: Abolitionist Tools for the New Jim Code. Newark, UNITED KINGDOM: Polity Press. http://ebookcentral.proquest.com/lib/washington/detail.action?docID=5820427.\n\n\nBonde, Sheila, and Paul Firenze. 2013. “A Framework for Making Ethical Decisions.” https://www.brown.edu/academics/science-and-technology-studies/framework-making-ethical-decisions.\n\n\nBooth, Robert. 2020. “Uber Drivers to Launch Legal Bid to Uncover App’s Algorithm.” http://www.theguardian.com/technology/2020/jul/20/uber-drivers-to-launch-legal-bid-to-uncover-apps-algorithm.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Conference on Fairness, Accountability and Transparency.” In, 77–91. PMLR. http://proceedings.mlr.press/v81/buolamwini18a.html.\n\n\nBuolamwini, Joy, Vicente Ordóñez, Jamie Morgenstern, and Erik Learned-Miller. 2020. “Facial Recognition Technologies: A Primer,” May. https://global-uploads.webflow.com/5e027ca188c99e3515b404b7/5ed1002058516c11edc66a14_FRTsPrimerMay2020.pdf.\n\n\nCampbell, Alexia Fernández. 2018. “How Tech Employees Are Pushing Silicon Valley to Put Ethics Before Profit.” Vox, October. https://www.vox.com/technology/2018/10/18/17989482/google-amazon-employee-ethics-contracts.\n\n\nCastelvecchi, Davide. 2020. “Is Facial Recognition Too Biased to Be Let Loose?” Nature 587 (7834): 347–49. https://doi.org/10.1038/d41586-020-03186-4.\n\n\nChen, Irene Y., Emma Pierson, Sherri Rose, Shalmali Joshi, Kadija Ferryman, and Marzyeh Ghassemi. 2021. “Ethical Machine Learning in Healthcare.” Annual Review of Biomedical Data Science 4 (1): null. https://doi.org/10.1146/annurev-biodatasci-092820-114757.\n\n\nChouldechova, Alexandra. 2017. “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.” Big Data 5 (2): 153–63. https://doi.org/10.1089/big.2016.0047.\n\n\nChowdhury, Rumman. 2021. “Sharing Learnings about Our Image Cropping Algorithm.” https://blog.twitter.com/engineering/en_us/topics/insights/2021/sharing-learnings-about-our-image-cropping-algorithm.\n\n\nCrawford, Kate. 2021. “State.” In, 181–209. Power, Politics, and the Planetary Costs of Artificial Intelligence. Yale University Press. https://doi.org/10.2307/j.ctv1ghv45t.9.\n\n\nEubanks, Virginia. 2018. “A Child Abuse Prediction Model Fails Poor Families.” Wired, January. https://www.wired.com/story/excerpt-from-automating-inequality/.\n\n\nFeathers, Todd. 2021. “Police Are Telling ShotSpotter to Alter Evidence From Gunshot-Detecting AI.” https://www.vice.com/en/article/qj8xbq/police-are-telling-shotspotter-to-alter-evidence-from-gunshot-detecting-ai.\n\n\nFriedman, Batya, and Helen Nissenbaum. 1996. “Bias in Computer Systems.” ACM Transactions on Information Systems 14 (3): 330347. https://doi.org/10.1145/230538.230561.\n\n\nFry, Hannah. 2019. “What Statistics Can and Can’t Tell Us about Ourselves.” The New Yorker, September. https://www.newyorker.com/magazine/2019/09/09/what-statistics-can-and-cant-tell-us-about-ourselves.\n\n\nGilman, Michele. 2020. “Poverty Lawgorithms.” https://datasociety.net/library/poverty-lawgorithms/.\n\n\nGreen, Ben. 2019. “The Innovative City: The Relationship Between Technical and Nontechnical Change in City Government.” In The Smart Enough City: Putting Technology in Its Place to Reclaim Our Urban Future. https://direct.mit.edu/books/book/4204/chapter/172388/The-Innovative-City-The-Relationship-between.\n\n\n———. 2020. “The False Promise of Risk Assessments: Epistemic Reform and the Limits of Fairness.” In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 594–606. FAT* ’20. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3351095.3372869.\n\n\nGupta, Abhishek, and Victoria Heath. 2020. “AI Ethics Groups Are Repeating One of Society’s Classic Mistakes.” MIT Technology Review, September. https://www.technologyreview.com/2020/09/14/1008323/ai-ethics-representation-artificial-intelligence-opinion/.\n\n\nGurley, Lauren Kaori. 2021. “Amazon Drivers Are Instructed to Drive Recklessly to Meet Delivery Quotas.” https://www.vice.com/en/article/xgxx54/amazon-drivers-are-instructed-to-drive-recklessly-to-meet-delivery-quotas.\n\n\nHeilweil, Rebecca. 2020. “Facebook Is Taking a Hard Look at Racial Bias in Its Algorithms.” Vox, July. https://www.vox.com/recode/2020/7/22/21334051/facebook-news-feed-instagram-algorithm-racial-bias-civil-rights-audit.\n\n\nHenriques-Gomes, Luke. 2019. “The Automated System Leaving Welfare Recipients Cut Off with Nowhere to Turn.” The Guardian, October. http://www.theguardian.com/technology/2019/oct/16/automated-messages-welfare-australia-system.\n\n\nHill, Kashmir. 2020. “Wrongfully Accused by an Algorithm.” The New York Times, June. https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html.\n\n\nHill, Kashmir, and Aaron Krolik. 2019. “How Photos of Your Kids Are Powering Surveillance Technology.” The New York Times, October. https://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-recognition.html.\n\n\nHooker, Sara. 2021. “Moving Beyond “Algorithmic Bias Is a Data Problem”.” Patterns 2 (4). https://doi.org/10.1016/j.patter.2021.100241.\n\n\nIngold, David, and Spencer Soper. 2016. “Amazon Doesn’t Consider the Race of Its Customers. Should It?” http://www.bloomberg.com/graphics/2016-amazon-same-day/.\n\n\nJeffries, Adrianne, Leon Yin, and Surya Mattu. 2020. “Swinging the Vote?” The Markup, February. https://themarkup.org/google-the-giant/2020/02/26/wheres-my-email.\n\n\nJiang, Liwei. 2021. “Towards Machine Ethics and Norms.” https://medium.com/ai2-blog/towards-machine-ethics-and-norms-d64f2bdde6a3.\n\n\nKearns, Michael, and Aaron Roth. 2019. “Introduction.” In The Ethical Algorithm: The Science of Socially Aware Algorithm Design. Oxford, UNITED STATES: Oxford University Press USA - OSO. http://ebookcentral.proquest.com/lib/washington/detail.action?docID=5905172.\n\n\nKirchner, Lauren. 2020. “When Zombie Data Costs You a Home.” The Markup, October. https://themarkup.org/locked-out/2020/10/06/zombie-criminal-records-housing-background-checks.\n\n\n———. 2021. “Powerful DNA Software Used in Hundreds of Criminal Cases Faces New Scrutiny.” The Markup, March. https://themarkup.org/news/2021/03/09/powerful-dna-software-used-in-hundreds-of-criminal-cases-faces-new-scrutiny.\n\n\nLum, Kristian, and Rumman Chowdhury. 2021. “What Is an ‘Algorithm’? It Depends Whom You Ask.” MIT Technology Review, February. https://www.technologyreview.com/2021/02/26/1020007/what-is-an-algorithm/.\n\n\nLum, Kristian, and William Isaac. 2016. “To Predict and Serve?” Significance 13 (5): 14–19. https://doi.org/https://doi.org/10.1111/j.1740-9713.2016.00960.x.\n\n\nMcVean, Ada. 2019. “40 Years of Human Experimentation in America: The Tuskegee Study.” https://www.mcgill.ca/oss/article/history/40-years-human-experimentation-america-tuskegee-study.\n\n\nMervis, Jeffery. 2019. “Can a Set of Equations Keep U.S. Census Data Private?” ScienceInsider. https://www.science.org/content/article/can-set-equations-keep-us-census-data-private.\n\n\nMIT Media Lab. 2018. “Gender Shades,” February. https://www.youtube.com/watch?v=TWWsW1w-BVo.\n\n\nMIT Technology Review. 2021. “Podcast: In Machines We Trust - Hired by an Algorithm,” June. https://www.youtube.com/watch?v=ztcVB_zh_M0.\n\n\nMitchell, Shira, Eric Potash, Solon Barocas, Alexander D’Amour, and Kristian Lum. 2021. “Algorithmic Fairness: Choices, Assumptions, and Definitions.” Annual Review of Statistics and Its Application 8 (1): null. https://doi.org/10.1146/annurev-statistics-042720-125902.\n\n\nMullainathan, Sendhil. 2019. “Biased Algorithms Are Easier to Fix Than Biased People.” The New York Times, December. https://www.nytimes.com/2019/12/06/business/algorithm-bias-fix.html.\n\n\nNIckelsburg, Monica. 2021. “Washington State Lawmakers Seek to Ban Government from Using Discriminatory AI Tech.” GeekWire, February. https://www.geekwire.com/2021/washington-state-lawmakers-seek-ban-government-using-ai-tech-discriminates/.\n\n\nNoorden, Richard Van. 2020. “The Ethical Questions That Haunt Facial-Recognition Research.” Nature 587 (7834): 354–58. https://doi.org/10.1038/d41586-020-03187-3.\n\n\nO’Neill, James. 2019. “How Facial Recognition Makes You Safer.” The New York Times, June. https://www.nytimes.com/2019/06/09/opinion/facial-recognition-police-new-york-city.html.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53. https://doi.org/10.1126/science.aax2342.\n\n\nOchigame, Rodrigo. 2020. “The Long History of Algorithmic Fairness.” https://phenomenalworld.org/analysis/long-history-algorithmic-fairness.\n\n\nOnuoha, Mimi. 2020. “When Proof Is Not Enough.” https://fivethirtyeight.com/features/when-proof-is-not-enough/.\n\n\nRaghavan, Manish, and Solon Barocas. 2019. “Challenges for Mitigating Bias in Algorithmic Hiring.” https://www.brookings.edu/research/challenges-for-mitigating-bias-in-algorithmic-hiring/.\n\n\nRaji, Deborah. 2019. “That’s Not Fair!” XRDS: Crossroads, The ACM Magazine for Students 25 (3): 44–48. https://doi.org/10.1145/3313127.\n\n\nRaji, Inioluwa Deborah, and Joy Buolamwini. 2019. “Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products.” In, 429435. AIES ’19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3306618.3314244.\n\n\nRichardson, Rashida. 2019. “Confronting Black Boxes: A Shadow Report of the New York City Automated Decision System Task Force.” https://ainowinstitute.org/ads-shadowreport-2019.html.\n\n\nSmith, Ben. 2021. “How TikTok Reads Your Mind.” The New York Times, December. https://www.nytimes.com/2021/12/05/business/media/tiktok-algorithm.html.\n\n\nSudhir, K., and Shyam Sunder. 2020. “What Happens When a Billion Identities Are Digitized?” https://insights.som.yale.edu/insights/what-happens-when-billion-identities-are-digitized.\n\n\nThe Financial Times. 2020. “The Uber Game.” https://ig.ft.com/uber-game.\n\n\nUpchurch, Tom. 2018. “To Work for Society, Data Scientists Need a Hippocratic Oath with Teeth.” Wired UK. https://www.wired.co.uk/article/data-ai-ethics-hippocratic-oath-cathy-o-neil-weapons-of-math-destruction.\n\n\nVarner, Maddy, and Aaron Sankin. 2020. “Suckers List: How Allstate’s Secret Auto Insurance Algorithm Squeezes Big Spenders.” The Markup, February. https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list.\n\n\nVox. 2021. “Are We Automating Racism?” March. https://www.youtube.com/watch?v=Ok5sKLXqynQ.\n\n\nWezerek, Gus, and David Van Riper. 2020. “Opinion | Changes to the Census Could Make Small Towns Disappear.” The New York Times, February. https://www.nytimes.com/interactive/2020/02/06/opinion/census-algorithm-privacy.html.\n\n\nWhitby, Andrew. 2020. “A Brief History of the Censusand How Covid-19 Could Change It.” Wired, April. https://www.wired.com/story/brief-history-census-how-covid-19-could-change-it/.\n\n\nWolford, Ben. 2018. “What Is GDPR, the EU’s New Data Protection Law?” https://gdpr.eu/what-is-gdpr/.\n\n\nWood, Alexandra, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi, James Honaker, Kobbi Nissim, David O’Brien, Thomas Steinke, and Salil Vadhan. 2018. “Differential Privacy: A Primer for a Non-Technical Audience.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3338027."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Instructions for Discussants\n\n\nResponse 1\n\n\nResponse 2\n\n\nResponse 3\n\n\nFinal project"
  }
]
[
  {
    "objectID": "readings.html",
    "href": "readings.html",
    "title": "Schedule",
    "section": "",
    "text": "References\n\nACLU of Washington. 2021. “Automated Decision Making Systems Are Making Some of the Most Important Life Decisions For You, but You Might Not Even Know It.” https://www.aclu-wa.org/story/automated-decision-making-systems-are-making-some-most-important-life-decisions-you-you-might.\n\n\nAllen Institute for AI. 2021. “Ask Delphi.” https://delphi.allenai.org/.\n\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. “Machine Bias.” https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing?token=jRdnwabwdw5HLiHY-R3nqWS5DOjEM7W-.\n\n\nArvind Narayanan. 2018. “Tutorial: 21 Fairness Definitions and Their Politics,” March. https://www.youtube.com/watch?v=jIXIuYdnyyk.\n\n\nBaio, Andy. 2022. “Exploring 12 Million of the 2.3 Billion Images Used to Train Stable Diffusion’s Image Generator.” https://waxy.org/2022/08/exploring-12-million-of-the-images-used-to-train-stable-diffusions-image-generator/.\n\n\nBembeneck, Emily, Rebecca Nissan, and Ziad Obermeyer. 2021. “To Stop Algorithmic Bias, We First Have to Define It.” https://www.brookings.edu/research/to-stop-algorithmic-bias-we-first-have-to-define-it/.\n\n\nBenjamin, Ruha. 2019. “Default Discrimination.” In Race After Technology: Abolitionist Tools for the New Jim Code. Newark, UNITED KINGDOM: Polity Press. http://ebookcentral.proquest.com/lib/washington/detail.action?docID=5820427.\n\n\nBonde, Sheila, and Paul Firenze. 2013. “A Framework for Making Ethical Decisions.” https://www.brown.edu/academics/science-and-technology-studies/framework-making-ethical-decisions.\n\n\nBooth, Robert. 2020. “Uber Drivers to Launch Legal Bid to Uncover App’s Algorithm.” http://www.theguardian.com/technology/2020/jul/20/uber-drivers-to-launch-legal-bid-to-uncover-apps-algorithm.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Conference on Fairness, Accountability and Transparency.” In, 77–91. PMLR. http://proceedings.mlr.press/v81/buolamwini18a.html.\n\n\nBuolamwini, Joy, Vicente Ordóñez, Jamie Morgenstern, and Erik Learned-Miller. 2020. “Facial Recognition Technologies: A Primer,” May. https://global-uploads.webflow.com/5e027ca188c99e3515b404b7/5ed1002058516c11edc66a14_FRTsPrimerMay2020.pdf.\n\n\nCampbell, Alexia Fernández. 2018. “How Tech Employees Are Pushing Silicon Valley to Put Ethics Before Profit.” Vox, October. https://www.vox.com/technology/2018/10/18/17989482/google-amazon-employee-ethics-contracts.\n\n\nCastelvecchi, Davide. 2020. “Is Facial Recognition Too Biased to Be Let Loose?” Nature 587 (7834): 347–49. https://doi.org/10.1038/d41586-020-03186-4.\n\n\nChen, Irene Y., Emma Pierson, Sherri Rose, Shalmali Joshi, Kadija Ferryman, and Marzyeh Ghassemi. 2021. “Ethical Machine Learning in Healthcare.” Annual Review of Biomedical Data Science 4 (1): null. https://doi.org/10.1146/annurev-biodatasci-092820-114757.\n\n\nChouldechova, Alexandra. 2017. “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.” Big Data 5 (2): 153–63. https://doi.org/10.1089/big.2016.0047.\n\n\n———. 2020. “Transparency and Simplicity in Criminal Risk Assessment.” Harvard Data Science Review 2 (1). https://doi.org/10.1162/99608f92.b9343eec.\n\n\nChowdhury, Rumman. 2021. “Sharing Learnings about Our Image Cropping Algorithm.” https://blog.twitter.com/engineering/en_us/topics/insights/2021/sharing-learnings-about-our-image-cropping-algorithm.\n\n\nCrawford, Kate. 2021. “State.” In, 181–209. Power, Politics, and the Planetary Costs of Artificial Intelligence. Yale University Press. https://doi.org/10.2307/j.ctv1ghv45t.9.\n\n\nEubanks, Virginia. 2018. “A Child Abuse Prediction Model Fails Poor Families.” Wired, January. https://www.wired.com/story/excerpt-from-automating-inequality/.\n\n\nFeathers, Todd. 2021. “Police Are Telling ShotSpotter to Alter Evidence From Gunshot-Detecting AI.” https://www.vice.com/en/article/qj8xbq/police-are-telling-shotspotter-to-alter-evidence-from-gunshot-detecting-ai.\n\n\nFriedman, Batya, and Helen Nissenbaum. 1996. “Bias in Computer Systems.” ACM Transactions on Information Systems 14 (3): 330347. https://doi.org/10.1145/230538.230561.\n\n\nGilman, Michele. 2020. “Poverty Lawgorithms.” https://datasociety.net/library/poverty-lawgorithms/.\n\n\nGreen, Ben. 2019. “The Innovative City: The Relationship Between Technical and Nontechnical Change in City Government.” In The Smart Enough City: Putting Technology in Its Place to Reclaim Our Urban Future. https://direct.mit.edu/books/book/4204/chapter/172388/The-Innovative-City-The-Relationship-between.\n\n\n———. 2020. “The False Promise of Risk Assessments: Epistemic Reform and the Limits of Fairness.” In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 594–606. FAT* ’20. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3351095.3372869.\n\n\nGupta, Abhishek, and Victoria Heath. 2020. “AI Ethics Groups Are Repeating One of Society’s Classic Mistakes.” MIT Technology Review, September. https://www.technologyreview.com/2020/09/14/1008323/ai-ethics-representation-artificial-intelligence-opinion/.\n\n\nGurley, Lauren Kaori. 2021. “Amazon Drivers Are Instructed to Drive Recklessly to Meet Delivery Quotas.” https://www.vice.com/en/article/xgxx54/amazon-drivers-are-instructed-to-drive-recklessly-to-meet-delivery-quotas.\n\n\nHeilweil, Rebecca. 2020. “Facebook Is Taking a Hard Look at Racial Bias in Its Algorithms.” Vox, July. https://www.vox.com/recode/2020/7/22/21334051/facebook-news-feed-instagram-algorithm-racial-bias-civil-rights-audit.\n\n\nHenriques-Gomes, Luke. 2019. “The Automated System Leaving Welfare Recipients Cut Off with Nowhere to Turn.” The Guardian, October. http://www.theguardian.com/technology/2019/oct/16/automated-messages-welfare-australia-system.\n\n\nHill, Kashmir. 2020. “Wrongfully Accused by an Algorithm.” The New York Times, June. https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html.\n\n\nHill, Kashmir, and Aaron Krolik. 2019. “How Photos of Your Kids Are Powering Surveillance Technology.” The New York Times, October. https://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-recognition.html.\n\n\nHooker, Sara. 2021. “Moving Beyond “Algorithmic Bias Is a Data Problem”.” Patterns 2 (4). https://doi.org/10.1016/j.patter.2021.100241.\n\n\nIngold, David, and Spencer Soper. 2016. “Amazon Doesn’t Consider the Race of Its Customers. Should It?” http://www.bloomberg.com/graphics/2016-amazon-same-day/.\n\n\nJackson, Eugenie, and Christina Mendoza. 2020. “Setting the Record Straight: What the COMPAS Core Risk and Need Assessment Is and Is Not.” Harvard Data Science Review 2 (1). https://doi.org/10.1162/99608f92.1b3dadaa.\n\n\nJeffries, Adrianne, Leon Yin, and Surya Mattu. 2020. “Swinging the Vote?” The Markup, February. https://themarkup.org/google-the-giant/2020/02/26/wheres-my-email.\n\n\nJiang, Liwei. 2021. “Towards Machine Ethics and Norms.” https://medium.com/ai2-blog/towards-machine-ethics-and-norms-d64f2bdde6a3.\n\n\nKearns, Michael, and Aaron Roth. 2019. “Introduction.” In The Ethical Algorithm: The Science of Socially Aware Algorithm Design. Oxford, UNITED STATES: Oxford University Press USA - OSO. http://ebookcentral.proquest.com/lib/washington/detail.action?docID=5905172.\n\n\nKirchner, Lauren. 2020. “When Zombie Data Costs You a Home.” The Markup, October. https://themarkup.org/locked-out/2020/10/06/zombie-criminal-records-housing-background-checks.\n\n\n———. 2021. “Powerful DNA Software Used in Hundreds of Criminal Cases Faces New Scrutiny.” The Markup, March. https://themarkup.org/news/2021/03/09/powerful-dna-software-used-in-hundreds-of-criminal-cases-faces-new-scrutiny.\n\n\nLum, Kristian, and Rumman Chowdhury. 2021. “What Is an ‘Algorithm’? It Depends Whom You Ask.” MIT Technology Review, February. https://www.technologyreview.com/2021/02/26/1020007/what-is-an-algorithm/.\n\n\nLum, Kristian, and William Isaac. 2016. “To Predict and Serve?” Significance 13 (5): 14–19. https://doi.org/https://doi.org/10.1111/j.1740-9713.2016.00960.x.\n\n\nMcVean, Ada. 2019. “40 Years of Human Experimentation in America: The Tuskegee Study.” https://www.mcgill.ca/oss/article/history/40-years-human-experimentation-america-tuskegee-study.\n\n\nMervis, Jeffery. 2019. “Can a Set of Equations Keep U.S. Census Data Private?” ScienceInsider. https://www.science.org/content/article/can-set-equations-keep-us-census-data-private.\n\n\nMIT Media Lab. 2018. “Gender Shades,” February. https://www.youtube.com/watch?v=TWWsW1w-BVo.\n\n\nMIT Technology Review. 2021. “Podcast: In Machines We Trust - Hired by an Algorithm,” June. https://www.youtube.com/watch?v=ztcVB_zh_M0.\n\n\nMitchell, Shira, Eric Potash, Solon Barocas, Alexander D’Amour, and Kristian Lum. 2021. “Algorithmic Fairness: Choices, Assumptions, and Definitions.” Annual Review of Statistics and Its Application 8 (1): null. https://doi.org/10.1146/annurev-statistics-042720-125902.\n\n\nMullainathan, Sendhil. 2019. “Biased Algorithms Are Easier to Fix Than Biased People.” The New York Times, December. https://www.nytimes.com/2019/12/06/business/algorithm-bias-fix.html.\n\n\nNIckelsburg, Monica. 2021. “Washington State Lawmakers Seek to Ban Government from Using Discriminatory AI Tech.” GeekWire, February. https://www.geekwire.com/2021/washington-state-lawmakers-seek-ban-government-using-ai-tech-discriminates/.\n\n\nNoorden, Richard Van. 2020. “The Ethical Questions That Haunt Facial-Recognition Research.” Nature 587 (7834): 354–58. https://doi.org/10.1038/d41586-020-03187-3.\n\n\nO’Neill, James. 2019. “How Facial Recognition Makes You Safer.” The New York Times, June. https://www.nytimes.com/2019/06/09/opinion/facial-recognition-police-new-york-city.html.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53. https://doi.org/10.1126/science.aax2342.\n\n\nOchigame, Rodrigo. 2020. “The Long History of Algorithmic Fairness.” https://phenomenalworld.org/analysis/long-history-algorithmic-fairness.\n\n\nOnuoha, Mimi. 2020. “When Proof Is Not Enough.” https://fivethirtyeight.com/features/when-proof-is-not-enough/.\n\n\nRaghavan, Manish, and Solon Barocas. 2019. “Challenges for Mitigating Bias in Algorithmic Hiring.” https://www.brookings.edu/research/challenges-for-mitigating-bias-in-algorithmic-hiring/.\n\n\nRaji, Deborah. 2019. “That’s Not Fair!” XRDS: Crossroads, The ACM Magazine for Students 25 (3): 44–48. https://doi.org/10.1145/3313127.\n\n\nRaji, Inioluwa Deborah, and Joy Buolamwini. 2019. “Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products.” In, 429435. AIES ’19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3306618.3314244.\n\n\nRichardson, Rashida. 2019. “Confronting Black Boxes: A Shadow Report of the New York City Automated Decision System Task Force.” https://ainowinstitute.org/ads-shadowreport-2019.html.\n\n\nRudin, Cynthia, Caroline Wang, and Beau Coker. 2020. “The Age of Secrecy and Unfairness in Recidivism Prediction.” Harvard Data Science Review 2 (1). https://doi.org/10.1162/99608f92.6ed64b30.\n\n\nSmith, Ben. 2021. “How TikTok Reads Your Mind.” The New York Times, December. https://www.nytimes.com/2021/12/05/business/media/tiktok-algorithm.html.\n\n\nstabilityai. n.d. “Stable Diffusion - a Hugging Face Space.” https://huggingface.co/spaces/stabilityai/stable-diffusion.\n\n\nSudhir, K., and Shyam Sunder. 2020. “What Happens When a Billion Identities Are Digitized?” https://insights.som.yale.edu/insights/what-happens-when-billion-identities-are-digitized.\n\n\nThe Financial Times. 2020. “The Uber Game.” https://ig.ft.com/uber-game.\n\n\nUpchurch, Tom. 2018. “To Work for Society, Data Scientists Need a Hippocratic Oath with Teeth.” Wired UK. https://www.wired.co.uk/article/data-ai-ethics-hippocratic-oath-cathy-o-neil-weapons-of-math-destruction.\n\n\nVarner, Maddy, and Aaron Sankin. 2020. “Suckers List: How Allstate’s Secret Auto Insurance Algorithm Squeezes Big Spenders.” The Markup, February. https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list.\n\n\nVox. 2021. “Are We Automating Racism?” March. https://www.youtube.com/watch?v=Ok5sKLXqynQ.\n\n\nWezerek, Gus, and David Van Riper. 2020. “Opinion | Changes to the Census Could Make Small Towns Disappear.” The New York Times, February. https://www.nytimes.com/interactive/2020/02/06/opinion/census-algorithm-privacy.html.\n\n\nWhitby, Andrew. 2020. “A Brief History of the Censusand How Covid-19 Could Change It.” Wired, April. https://www.wired.com/story/brief-history-census-how-covid-19-could-change-it/.\n\n\nWolford, Ben. 2018. “What Is GDPR, the EU’s New Data Protection Law?” https://gdpr.eu/what-is-gdpr/.\n\n\nWood, Alexandra, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi, James Honaker, Kobbi Nissim, David O’Brien, Thomas Steinke, and Salil Vadhan. 2018. “Differential Privacy: A Primer for a Non-Technical Audience.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3338027."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Response 1 (due 10/20)\n----------\nBelow are the instructions for the assignments used in Winter 2022. For Fall 2022, you can expect similar types of assignments (to be posted).\n\n\nResponse 1 WINTER 2022\n\n\nResponse 2 WINTER 2022\n\n\nResponse 3 WINTER 2022\n\n\nFinal project WINTER 2022"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 303",
    "section": "",
    "text": "This course was originally developed in collaboration with Sarah Teichman."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "files/responses/02_fairness.html",
    "href": "files/responses/02_fairness.html",
    "title": "Long Response 2: Statistical Definitions of Fairness",
    "section": "",
    "text": "In this assignment, you will apply what we’ve discussed about statistical definitions of fairness. Read this article and this article detailing the use of algorithms to predict student success in higher education.\nConsider an algorithm that outputs a score \\(S\\) that measures a student’s risk of not graduating on time. Inputs \\(X\\) can include grades and other data discussed in the above articles. Suppose this score \\(S\\) is used to make predictions about \\(Y\\), where \\(Y=1\\) is the student graduates on time and \\(Y=0\\) if not. Suppose \\(R\\) describes race. Using the definitions in Chouldechova (2017), answer the following questions."
  },
  {
    "objectID": "files/responses/02_fairness.html#instructions",
    "href": "files/responses/02_fairness.html#instructions",
    "title": "Long Response 2: Statistical Definitions of Fairness",
    "section": "Instructions",
    "text": "Instructions\n\n(2 points) Explain, in plain language, what calibration means in this context.\n(2 points) Explain, in plain language, what predictive parity means in this context.\n(2 points) Explain, in plain language, what error rate balance means in this context.\n(3 points) Do the algorithms described in the articles above violate any of the above definitions of fairness?\n(3 points) In your opinion, which of the three defintions of fairness above would be the most important to satisfy in this context? Why?\n(3 points) If such an algorithm were shown to satisfy your preferred definition of fairness, would you be comfortable with its use in this context? What other ethical concerns should we consider?"
  },
  {
    "objectID": "files/responses/03_resistance.html",
    "href": "files/responses/03_resistance.html",
    "title": "Long Response 3: Forms of Resistance",
    "section": "",
    "text": "Length: 3-5 pages, 12 point font, double spaced."
  },
  {
    "objectID": "files/responses/03_resistance.html#overview",
    "href": "files/responses/03_resistance.html#overview",
    "title": "Long Response 3: Forms of Resistance",
    "section": "Overview",
    "text": "Overview\nWhen ethical concerns arise as a result of algorithmic decision-making systems, how can we respond? In this long response assignment, you will select and study one remedy or form of resistance to algorithmic decision-making. Some examples to consider:\n\nRegulation via legislation\nConsumer boycotts\nWorker-led organizing (ex. walkouts)\nAlgorithmic fairness research\n\nIn your long response, you should outline your chosen form of resistance, identify actual examples where it has been applied, and discuss when and where it could be an effective response to abuses of algorithmic decision-making."
  },
  {
    "objectID": "files/responses/03_resistance.html#instructions",
    "href": "files/responses/03_resistance.html#instructions",
    "title": "Long Response 3: Forms of Resistance",
    "section": "Instructions",
    "text": "Instructions\n\nYour long response should include the following sections:\n\nIntroduction (3 points): Introduce and define your form of resistance. Who are the main actors or participants?\nHistorical examples (3 points): Identify at least two examples where your form of resistance has been used to respond to algorithmic decision-making and discuss whether it was effective.\nPossibilities (3 points): What kind of ethical concerns can be addressed via your selected form of resistance? Identify ways in which your selected form of resistance could be effective.\nShortcomings (3 points): What kind of ethical concerns cannot be addressed via your selected form of resistance? Identify ways in which your selected form of resistance may not address all possible concerns?\n\nFinally, your long response will also be graded on the following criterion:\n\nPresentation/exposition (3 points): Submissions should be carefully edited, with citations for all resources used."
  },
  {
    "objectID": "files/responses/01_definitions-2022.html",
    "href": "files/responses/01_definitions-2022.html",
    "title": "Long Response 1: Case Studies",
    "section": "",
    "text": "Length: 3-5 pages, 12 point font, double spaced."
  },
  {
    "objectID": "files/responses/01_definitions-2022.html#overview",
    "href": "files/responses/01_definitions-2022.html#overview",
    "title": "Long Response 1: Case Studies",
    "section": "Overview",
    "text": "Overview\nIn this first long response assignment, you will select and study one data-driven decision-making system. For the purpose of this assignment, we define an data-driven decision-making system as a system or process that uses computation and data to make automated predictions or decisions about individual people or groups of people. For example, although Newton’s method is an algorithm, it is not in itself an data-driven decision system. Some examples that would be appropriate include recommendation systems used by technology and social media companies, diagnostic algorithms used in medicine, or predictive screening tools used in education and by police.\nImagine that as a class, we are constructing a field guide/encyclopedia/guidebook that could be used as a reference for anyone interested in learning about algorithms in use today. In your response, you will briefly describe the design and purpose of your data-driven decision-making system, explain how it relates to ideas discussed in class, and identify potential ethical concerns related to its use. Your response should be concise, precise, and accessible to a general audience.\nYour long response should take the form of a brief blog post or encyclopedia article designed to introduce your topic to an audience of your peers."
  },
  {
    "objectID": "files/responses/01_definitions-2022.html#instructions",
    "href": "files/responses/01_definitions-2022.html#instructions",
    "title": "Long Response 1: Case Studies",
    "section": "Instructions",
    "text": "Instructions\n\nIdentify a decision-making system for your assignment and compile references you can draw on for your long response.\nYour long response should include the following sections. Your response will be graded on your ability to cover the material outlined below in a clear, precise, and complete manner.\n\nIntroduction and background (4 points): Introduce your decision-making system and relevant background information. Who created it and what purpose does it serve? Who are its users? Who are its subjects? How were decisions made before implementing this system? Are there related data-driven decision-making systems that we should know about?\nHow it works (4 points): Explain how your chosen example works. What are the inputs and outputs? If technical details are unavailable, explain why they are unavailable and to the best of your ability, use available information to explain how such algorithms typically work.\nEthical issues (4 points): Identify potential ethical concerns and harms related to your decision-making system. If there are harms associated with your decision-making system, who is affected? Be sure to discuss potential causes.\nRemedies and conclusions (4 points): For each ethical concern/harm raised, be sure to identify potential remedies if there are any. What do you think should be done? Provide a clear set of suggestions.\n\nFinally, your long response will also be graded on the following criterion:\n\nPresentation/exposition (4 points): Submissions should be carefully edited with minimal typos/grammatical mistakes. Citations should be provided for all resources used.\n\nFor extra credit, you may find a partner in the class and peer edit each other’s responses. In addition to the final drafts of your long responses, each of you should submit both the initial version of your paper, with your partner’s comments as well as the initial version of your partner’s paper, with your comments. In order to receive extra credit, you must seriously engage with your partner’s writing and their suggestions for your paper. Simply completing the requirements is not enough.\n\nComments (0.5 point extra credit): Provide constructive feedback—which parts require clarifying or editing? In addition to line edits, you should provide a written summary of your feedback including at least two substantive critiques/suggestions for improvement.\nEdits (0.5 point extra credit): Respond to and/or incorporate your partner’s suggestions."
  },
  {
    "objectID": "files/responses/01_definitions.html",
    "href": "files/responses/01_definitions.html",
    "title": "Long Response 1: Case Studies",
    "section": "",
    "text": "Length: 3-5 pages, 12 point font, double spaced."
  },
  {
    "objectID": "files/responses/01_definitions.html#overview",
    "href": "files/responses/01_definitions.html#overview",
    "title": "Long Response 1: Case Studies",
    "section": "Overview",
    "text": "Overview\nIn this first long response assignment, you will select and study one data-driven decision system. For the purpose of this assignment, we define an data-driven decision system as a system or process that uses computation and data to make automated predictions and decisions about individual people or groups of people. For example, although Newton’s method is an algorithm, it is not in itself an data-driven decision system. Some examples that would be appropriate include recommendation systems used by technology and social media companies, diagnostic algorithms used in medicine, or predictive screening tools used in education and by the police. In this long response, you will briefly describe the design and purpose of your data-driven decision system, explain how it relates to ideas discussed in class, and identify potential ethical concerns related to its use. Your response should be concise, precise, and accessible to a general audience."
  },
  {
    "objectID": "files/responses/01_definitions.html#instructions",
    "href": "files/responses/01_definitions.html#instructions",
    "title": "Long Response 1: Case Studies",
    "section": "Instructions",
    "text": "Instructions\n\nIdentify a decision system for your assignment and compile references you can draw on for your long response.\nYour long response should include the following sections:\n\nIntroduction (3 points): Introduce your decision system. What purpose does it serve? Who created it? Who are its users? Who are its subjects?\nOverview of algorithm (3 points): What, if any, algorithms/automated systems are involved? Why do these satisfy the definition of algorithm? What are the inputs and outputs?\nTechnical information (3 points): Explain how the algorithms work. If technical details are unavailable, explain why they are unavailable and to the best of your ability, use available information to explain how such algorithms typically work.\nEthical issues (3 points): Identify potential ethical concerns and harms related to your decision system. For each ethical concern/harm raised, be sure to identify potential remedies if there are any.\n\nFinally, your long response will also be graded on the following criterion:\n\nPresentation/exposition (3 points): Submissions should be carefully edited with minimal typos/grammatical mistakes. Citations should be provided for all resources used."
  },
  {
    "objectID": "files/responses/00_pre-class.html",
    "href": "files/responses/00_pre-class.html",
    "title": "Pre-class survey",
    "section": "",
    "text": "What is the name we should call you/how should we refer to you?\nWhat is your (intended) major?\nPlease explain what you hope to take away from this course.\nDo my current proposed office hour times work for you? If not, please provide some general availability.\nIs there anything else I should know that might help you succeed in this course?"
  },
  {
    "objectID": "files/responses/01_definitions-alt.html",
    "href": "files/responses/01_definitions-alt.html",
    "title": "Long Response 1: Case Studies",
    "section": "",
    "text": "Length: 3-5 pages, 12 point font, double spaced."
  },
  {
    "objectID": "files/responses/01_definitions-alt.html#overview",
    "href": "files/responses/01_definitions-alt.html#overview",
    "title": "Long Response 1: Case Studies",
    "section": "Overview",
    "text": "Overview\nIn this first long response assignment, you will select and study one data-driven decision-making system. For the purpose of this assignment, we define an data-driven decision-making system as a system or process that uses computation and data to make automated predictions or decisions about individual people or groups of people. For example, although Newton’s method is an algorithm, it is not in itself an data-driven decision system. Some examples that would be appropriate include recommendation systems used by technology and social media companies, diagnostic algorithms used in medicine, or predictive screening tools used in education and by police.\nImagine that as a class, we are constructing a field guide/encyclopedia/guidebook that could be used as a reference for anyone interested in learning about algorithms in use today. In your response, you will briefly describe the design and purpose of your data-driven decision-making system, explain how it relates to ideas discussed in class, and identify potential ethical concerns related to its use. Your response should be concise, precise, and accessible to a general audience.\nYour long response should take the form of a brief blog post or encyclopedia article designed to introduce your topic to an audience of your peers."
  },
  {
    "objectID": "files/responses/01_definitions-alt.html#instructions",
    "href": "files/responses/01_definitions-alt.html#instructions",
    "title": "Long Response 1: Case Studies",
    "section": "Instructions",
    "text": "Instructions\n\nIdentify a decision-making system for your assignment and compile references you can draw on for your long response.\nYour long response should include the following sections. Your response will be graded on your ability to cover the material outlined below in a clear, precise, and complete manner.\n\nIntroduction and background (4 points): Introduce your decision-making system and relevant background information. Who created it and what purpose does it serve? Who are its users? Who are its subjects? How were decisions made before implementing this system? Are there related data-driven decision-making systems that we should know about?\nHow it works (4 points): Explain how your chosen example works. What are the inputs and outputs? If technical details are unavailable, explain why they are unavailable and to the best of your ability, use available information to explain how such algorithms typically work.\nEthical issues (4 points): Identify potential ethical concerns and harms related to your decision-making system. If there are harms associated with your decision-making system, who is affected? Be sure to discuss potential causes.\nRemedies and conclusions (4 points): For each ethical concern/harm raised, be sure to identify potential remedies if there are any. What do you think should be done? Provide a clear set of suggestions.\n\nFinally, your long response will also be graded on the following criterion:\n\nPresentation/exposition (4 points): Submissions should be carefully edited with minimal typos/grammatical mistakes. Citations should be provided for all resources used.\n\nFor extra credit, you may find a partner in the class and peer edit each other’s responses. In addition to the final drafts of your long responses, each of you should submit both the initial version of your paper, with your partner’s comments as well as the initial version of your partner’s paper, with your comments. In order to receive extra credit, you must seriously engage with your partner’s writing and their suggestions for your paper. Simply completing the requirements is not enough.\n\nComments (0.5 point extra credit): Provide constructive feedback—which parts require clarifying or editing? In addition to line edits, you should provide a written summary of your feedback including at least two substantive critiques/suggestions for improvement.\nEdits (0.5 point extra credit): Respond to and/or incorporate your partner’s suggestions."
  },
  {
    "objectID": "files/discussant.html",
    "href": "files/discussant.html",
    "title": "Instructions for Discussants",
    "section": "",
    "text": "Each student will be designated as one of the discussants for one class section this quarter. As a discussant, you will help me facilitate the class by outlining the key ideas in each reading and preparing discussion questions. At times, I may also call on you to briefly explain some of the ideas in the readings or to share your thoughts or opinions. Although you will be expected to read and prepare carefully, you are not expected to be able to answer every question and you will not have to be the only one speaking. Above all, I hope to see that you are making an effort to lead the discussion and to engage with questions: do not worry if you forget something from the reading or do not have the answer for a particular question.\nIf you are the discussant for the same class as another student, I encourage you to contact them and discuss the readings/questions ahead of time."
  },
  {
    "objectID": "files/discussant.html#instructions",
    "href": "files/discussant.html#instructions",
    "title": "Instructions for Discussants",
    "section": "Instructions",
    "text": "Instructions\n\nCarefully read the texts for your assigned class section and take notes on the key ideas and themes.\n(5 points) Prepare two discussion questions that you would be willing to ask your classmates. At least one of these questions should connect multiple of the readings. A good discussion question is one that does not necessarily have one correct answer and should require some reflection to answer. Submit to Canvas a pdf with your two questions and your answers for each. A paragraph for each answer should be enough. Full credit will be given for questions and responses that demonstrate understanding of the readings and that spark thoughtful discussions.\n(5 points) On the day of your assigned class section, attend class and participate by sharing your discussion questions and responding to the instructor’s questions. If you are unable to attend in person, contact me ahead of time and we will make arrangements for you to attend virtually or to meet individually with me to complete this portion of the assignment.\n(1 point extra credit, optional) Identify an additional text/video/resource related to the readings for your assigned class and submit to Canvas a pdf with a brief summary (1-2 paragraphs) of your reading and how it relates to the other readings for the class."
  },
  {
    "objectID": "files/final-project.html#overview",
    "href": "files/final-project.html#overview",
    "title": "Final Project",
    "section": "Overview",
    "text": "Overview\nFor your final project, you will work in groups of up to three to research a topic related to the course material, develop and give a presentation on your findings, and write a brief report explaining your topic in an engaging and accessible manner.\n\nOption 1a: Imagine you are a consultant hired to audit automated decision systems. Choose a particular prediction algorithm/decision-making algorithm used by a company or a governmental agency. Identify potential ethical concerns and provide concrete recommendations for mitigating potential problems. Create a report/presentation detailing your findings.\nOption 1b: Imagine you are a researcher leading a government task force on accountability in algorithmic decision-making. Choose a particular prediction/decision-making problem for which algorithms are being used or developed. Identify potential ethical concerns and provide concrete recommendations for mitigating potential problems. Create a report/presentation detailing your findings.\nOption 2: Choose your own topic (have it approved by me before starting). Be creative–above all, you are encouraged to pursue topics that excite you.\n\nProject grades will be based on depth of analysis, research beyond course readings, quality of presentation, organization, and originality."
  },
  {
    "objectID": "files/final-project.html#part-one-project-proposal-3-points-due-28",
    "href": "files/final-project.html#part-one-project-proposal-3-points-due-28",
    "title": "Final Project",
    "section": "Part One: Project Proposal (3 points), due 2/8",
    "text": "Part One: Project Proposal (3 points), due 2/8\nYou will submit a 1-2 page project proposal with the following information:\n\nA brief description of your topic/algorithm/prediction problem.\nA list of your group members.\nAn overview of the main research questions you hope to explore through your project.\nA list of at least five references you plan to consult for your research.\nAny questions you have for me."
  },
  {
    "objectID": "files/final-project.html#part-two-presentation-15-points",
    "href": "files/final-project.html#part-two-presentation-15-points",
    "title": "Final Project",
    "section": "Part Two: Presentation (15 points)",
    "text": "Part Two: Presentation (15 points)\n10 minutes + 5 minutes for discussion.\nIf you select Option 1, you should prepare as if you are presenting your final recommendations to a client. In particular, you should be sure to cover the following areas:\n\nIntroduction: Introduce and define your topic. Provide an example illustrating how the algorithms you study are intended to be used.\nEthical concerns: What are potential ethical concerns related to your topic? If there are existing criticisms of your algorithm, be sure to discuss them.\nRecommendations: Describe any existing solutions that are being tried and whether they have been effective. What more could be done to address these concerns? For each ethical concern, be sure to provide at least one concrete recommendation for what could be done."
  },
  {
    "objectID": "files/final-project.html#part-three-final-report-15-points",
    "href": "files/final-project.html#part-three-final-report-15-points",
    "title": "Final Project",
    "section": "Part Three: Final Report (15 points)",
    "text": "Part Three: Final Report (15 points)\n5-7 pages. Other formats are acceptable with approval of the instructor, including podcast, video, website.\nPresent a final report expanding on your presentation. You do not have to write this in the form of an essay, but you should present your findings in a clear and organized way. In addition to the topics listed above, you should cover the following:\n\nTechnical details: How does your algorithm/group of algorithms typically work? What are the inputs and outputs? Where possible, provide details on the actual methods used, but keep the explanation at a level that is accessible to your client.\nConnections: If you are studying one particular algorithm, discuss any other competing algorithms that are used in your setting and whether they have the same issues. If you are studying a group of algorithms, discuss any other decision-making problems where similar ethical concerns arise. If possible, connect your topic to readings/ideas covered in class."
  },
  {
    "objectID": "files/final-project.html#part-four-reflection-and-self-evaluation-2-points.",
    "href": "files/final-project.html#part-four-reflection-and-self-evaluation-2-points.",
    "title": "Final Project",
    "section": "Part Four: Reflection and Self-evaluation (2 points).",
    "text": "Part Four: Reflection and Self-evaluation (2 points).\n1-2 pages. Answer the following questions:\n\nAt the beginning of the class, we discussed techno-optimism and techno-pessimism. What, if anything, about your answer has changed? What hasn’t changed?\nWhat is one thing that you feel you did well in this class? What is one area in which you wish to improve in the future?\nWhat, if any, topics did you enjoy reading about and discussing most? What, if any, topics did you not enjoy discussing?"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This syllabus is subject to change.\n\nCourse Information\nInstructor: Peter Gao (petergao at uw dot edu)\nTime: Tuesdays and Thursdays, 4:00 - 5:20pm\nLocation: GUG 204\nOffice hours: Tuesday/Thursday 3-3:50pm on Zoom or in-person (PDL C-316).\nNote: When emailing the instructor, please include “[STAT 303]” at the beginning of the email header! It will help us respond to you faster.\n\n\nOverview\nUsing examples from medicine, education, and criminal justice, this course surveys ethical & social implications of the design, implementation, & interpretation of statistical decision-making algorithms. Students will examine how algorithms interact with social categories including race, class, & gender, preserving or reshaping existing inequities. Students will evaluate statistical frameworks for balancing fairness & privacy with efficiency.\nCourse objectives\n\nIdentify and discuss ethical considerations related to the design, implementation, and results of decision making algorithms.\nExamine historical uses of data in decision making and draw connections to modern algorithmic decision making systems in fields including medicine, education, and criminal justice.\nUnderstand and critique statistical approaches to understanding fairness and privacy, especially as they pertain to data-driven decision making.\nPractice communicating about ethical considerations in data science through writing, discussion, and presentation.\n\n\n\nOutline\nIn this course, we will cover the following:\n\nIntroduction to ethics and algorithms\nAlgorithmic fairness and bias\nFacial recognition technologies\nAlgorithms in the public and private spheres\nPrivacy and surveillance\nPaths forward\nFinal Presentations\n\n\n\nCourse structure\n\nIn-class Discussion (10%): You will be expected to attend class regularly and to participate in class activities and discussions. Please try to bring something to each class: a question, an idea, a connection, etc. In addition, you will serve as a designated discussant for two class sessions during the quarter. As a discussant, you will prepare discussion questions in response to readings and be called upon to respond to questions that arise during class. More detailed instructions will be provided at the start of the quarter.\nIn-class Work (10%): In order to give students the flexibility to miss class as personal/health matters arise, attendance will not contribute to final grades. Instead, students will be asked to complete brief in-class assignments, which students may submit in person or online if they are unable to attend. You will receive zero, half, or full credit based upon completeness and depth of your responses.\nLong Responses (45%): Throughout the quarter, students will complete three writing assignments. These assignments ask students to write brief memos that could be used to communicate about issues related to algorithmic decision systems to a general public. These responses will be graded according to completeness and depth of your work.\n\nResponse 1 asks students to study an algorithm and identify potential ethical concerns.\nResponse 2 concerns statistical frameworks for assessing algorithmic bias and fairness.\nResponse 3 concerns forms of resistance to algorithmic decision systems.\n\nFinal Project (35%): In groups or by yourself, you will select a topic from class and extend our discussion (ex. By further examining the human impacts of a given algorithm). Halfway through the course, you will submit a final project proposal. During the 10th week, you will submit a written report and self assessment and present a brief talk (< 10 minutes) in-class.\n\nObtaining a good grade in this class will depend on your ability to stay organized and complete readings and assignments on time, but if you attend class with the goal of helping us to build a healthy discussion, you can and should be successful.\nIn general, the late policy is as follows: Any assignment that is received late but less than 24 hours late will receive a grade penalty of 25%. Any assignment that is received 24–48 hours late will receive a grade penalty of 50%. Assignments will not be accepted more than 48 hours late. That said, if you communicate directly with me before an assignment is due, I will often be willing to relax a deadline.\n\nExpectations\nThe COVID-19 pandemic has and will continue to present many of us with unforeseen difficulties. I encourage all of you to prioritize the health and safety of yourselves and those around you and would be happy to make accommodations that help you to do so. In addition, I hope that we can be patient with one another as we begin transitioning back to in-person learning, as it is likely that not everything will go to plan. Please feel free to reach out to me via email at any point to discuss any concerns you may have about the course.\n\n\nKeeping each other safe\nI am thrilled to be teaching in person and I hope you are excited to be back on campus as well. As we return to physical classrooms, please be respectful of your classmates’ boundaries and precautions–we are all readjusting to in-person learning. In addition, I hope we will all make every effort to keep ourselves and our classmates safe. If you test positive or are exposed to possible infection, I encourage you to err on the side of caution with regards to attending classes and would be happy to make accommodations that allow you to do so. I will make high-quality recordings of lecture and additional office hours available to students that are absent due to quarantine.\n\n\n\nClassroom environment\n\nNames & Pronouns\nEveryone deserves to be addressed as they would like. Feel free to send us your preferred name and correct pronouns at any time.\n\n\nFeedback\nI encourage and appreciate your feedback throughout the quarter. You are welcome to provide feedback on any aspect of the course at any time via email or in person. If you would prefer to do so confidentially, you can do so through the form here.\n\n\nParticipation expectations\nAbove all, we are trying to build a space for spirited discussion and learning. Come to class prepared to share what you have learned and to ask questions of us and your classmates. To be more specific:\n\nRead and review the assigned texts before class. Every class, try to bring something to contribute. What kind of something? Bring a question to ask the class, an idea to share, or a connection you drew to current events or other courses you have taken. If you loved a reading–great! We want to hear about it. If you hated a reading or think that a writer has it all wrong, also great!\nWe hope that you will feel comfortable taking risks in discussion. You do not always have to have the “right” answers or to prepare exactly what you want to say before you raise your hand. We are here to figure it out together, but you must take the first step by beginning to speak.\nListen to your classmates and really consider what they have to say. We are all here to learn–challenge each other by asking questions or building on each other’s ideas. Along these lines: don’t hog the mic! Give each other time to think and speak. It is our collective responsibility to help each other feel comfortable sharing our thoughts and ideas in the class.\nYou may sometimes feel that you do not have anything to contribute to a discussion. This is okay! We trust your judgment: sometimes the best way to contribute is by listening and taking some time to process your own thoughts.\n\n\n\nAcademic misconduct\nAcademic integrity is essential to this course and to your learning. On certain assignments, collaboration is allowed and encouraged when following the collaboration policy outlined above. Violations of the academic integrity policy include but are not limited to: copying from a peer, collaborating where it is not allowed, copying from an online resource, using a solutions manual, and using resources from a previous iteration of the course. Anything found in violation of this policy will be automatically given a score of 0 with no exceptions. If the situation merits, it will also be reported to the UW Student Conduct Office, at which point it will be out of my hands. If you have any questions about this policy, please do not hesitate to reach out and ask.\nThe university’s policy on plagiarism and academic misconduct is a part of the Student Conduct Code, which cites the definition of academic misconduct in the WAC 478-121. (WAC is an abbreviation for the Washington Administrative Code, the set of state regulations for the university. The entire chapter of the WAC on the student conduct code is here http://www.washington.edu/admin/rules/policies/WAC/478-121TOC.html) According to this section of the WAC, academic misconduct includes:\n“Cheating”—such as “unauthorized assistance in taking quizzes”, “Falsification” “which is the intentional use or submission of falsified data, records, or other information including, but not limited to, records of internship or practicum experiences or attendance at any required event(s), or scholarly research”; and “Plagiarism” which includes “[t]he use, by paraphrase or direct quotation, of the published or unpublished work of another person without full and clear acknowledgment.”\nThe UW Libraries have a useful guide for students at http://www.lib.washington.edu/teaching/plagiarism Students found to have engaged in academic misconduct may receive a zero on the assignment (or other possible outcome).\n\n\nConduct\nThe University of Washington Student Conduct Code (WAC 478-121) defines prohibited academic and behavioral conduct and describes how the University holds students accountable as they pursue their academic goals. Allegations of misconduct by students may be referred to the appropriate campus office for investigation and resolution. More information can be found online at https://www.washington.edu/studentconduct/.\n\n\nDisability Resources\nYour experience in this class is important to me. It is the policy and practice of the University of Washington to create inclusive and accessible learning environments consistent with federal and state law. If you have already established accommodations with Disability Resources for Students (DRS), please activate your accommodations via myDRS so we can discuss how they will be implemented in this course.\nIf you have not yet established services through DRS, but have a temporary health condition or permanent disability that requires accommodations (conditions include but not limited to; mental health, attention-related, learning, vision, hearing, physical or health impacts), contact DRS directly to set up an Access Plan. DRS facilitates the interactive process that establishes reasonable accommodations. Contact DRS at http://depts.washington.edu/uwdrs/\n\n\nDiversity, equity and inclusion\nDiverse backgrounds, embodiments, and experiences are essential to the critical thinking endeavor at the heart of university education. Therefore, I expect you to follow the UW Student Conduct Code in your interactions with your colleagues and me in this course by respecting the many social and cultural differences among us, which may include, but are not limited to: age, cultural background, disability, ethnicity, family status, gender identity and presentation, citizenship and immigration status, national origin, race, religious and political beliefs, sex, sexual orientation, socioeconomic status, and veteran status.\n\n\nReligious accommodations\nWashington state law requires that UW develop a policy for accommodation of student absences or significant hardship due to reasons of faith or conscience, or for organized religious activities. The UW’s policy, including more information about how to request an accommodation, is available at Religious Accommodations Policy (https://registrar.washington.edu/staffandfaculty/religious-accommodations-policy/). Accommodations must be requested within the first two weeks of this course using the Religious Accommodations Request form (https://registrar.washington.edu/students/religious-accommodations-request/).\n\n\nStudent privacy\nNote that the software used in this class (e.g. Canvas, Zoom, Panopto) when used with our UW Net IDs, are FERPA compliant (https://registrar.washington.edu/students/ferpa/). This means they do not monitor student use of their service and they do not share student data with third parties.\nSharing recordings and other class materials outside of class that include personally identifiable student information without the written consent of those students is a violation of FERPA. State law requires consent from people to be recorded (https://apps.leg.wa.gov/rcw/default.aspx?cite=9.73.030), please note that (1) that your participation in this class indicates your consent for course activities to be recorded, (2) you are not permitted to make your own recordings without consent from the instructor and everyone else involved, and (3) that the instructor’s recordings will be available for later playback only to students taking the course. For more information about privacy concerns, review the UW Privacy Office policies (https://privacy.uw.edu/policies/best-practices-online-conferencing/), or contact Helen Garrett, the UW’s FERPA Officer.\n\n\n\nLinks and other resources\n\nCanvasPage"
  },
  {
    "objectID": "readings_alt.html",
    "href": "readings_alt.html",
    "title": "Readings",
    "section": "",
    "text": "References\n\nACLU of Washington. 2021. “Automated Decision Making Systems Are Making Some of the Most Important Life Decisions For You, but You Might Not Even Know It.” https://www.aclu-wa.org/story/automated-decision-making-systems-are-making-some-most-important-life-decisions-you-you-might.\n\n\nAllen Institute for AI. 2021. “Ask Delphi.” https://delphi.allenai.org/.\n\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. “Machine Bias.” https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing?token=jRdnwabwdw5HLiHY-R3nqWS5DOjEM7W-.\n\n\nArvind Narayanan. 2018. “Tutorial: 21 Fairness Definitions and Their Politics,” March. https://www.youtube.com/watch?v=jIXIuYdnyyk.\n\n\nBaio, Andy. 2022. “Exploring 12 Million of the 2.3 Billion Images Used to Train Stable Diffusion’s Image Generator.” https://waxy.org/2022/08/exploring-12-million-of-the-images-used-to-train-stable-diffusions-image-generator/.\n\n\nBembeneck, Emily, Rebecca Nissan, and Ziad Obermeyer. 2021. “To Stop Algorithmic Bias, We First Have to Define It.” https://www.brookings.edu/research/to-stop-algorithmic-bias-we-first-have-to-define-it/.\n\n\nBenjamin, Ruha. 2019. “Default Discrimination.” In Race After Technology: Abolitionist Tools for the New Jim Code. Newark, UNITED KINGDOM: Polity Press. http://ebookcentral.proquest.com/lib/washington/detail.action?docID=5820427.\n\n\nBonde, Sheila, and Paul Firenze. 2013. “A Framework for Making Ethical Decisions.” https://www.brown.edu/academics/science-and-technology-studies/framework-making-ethical-decisions.\n\n\nBooth, Robert. 2020. “Uber Drivers to Launch Legal Bid to Uncover App’s Algorithm.” http://www.theguardian.com/technology/2020/jul/20/uber-drivers-to-launch-legal-bid-to-uncover-apps-algorithm.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Conference on Fairness, Accountability and Transparency.” In, 77–91. PMLR. http://proceedings.mlr.press/v81/buolamwini18a.html.\n\n\nBuolamwini, Joy, Vicente Ordóñez, Jamie Morgenstern, and Erik Learned-Miller. 2020. “Facial Recognition Technologies: A Primer,” May. https://global-uploads.webflow.com/5e027ca188c99e3515b404b7/5ed1002058516c11edc66a14_FRTsPrimerMay2020.pdf.\n\n\nCampbell, Alexia Fernández. 2018. “How Tech Employees Are Pushing Silicon Valley to Put Ethics Before Profit.” Vox, October. https://www.vox.com/technology/2018/10/18/17989482/google-amazon-employee-ethics-contracts.\n\n\nCastelvecchi, Davide. 2020. “Is Facial Recognition Too Biased to Be Let Loose?” Nature 587 (7834): 347–49. https://doi.org/10.1038/d41586-020-03186-4.\n\n\nChen, Irene Y., Emma Pierson, Sherri Rose, Shalmali Joshi, Kadija Ferryman, and Marzyeh Ghassemi. 2021. “Ethical Machine Learning in Healthcare.” Annual Review of Biomedical Data Science 4 (1): null. https://doi.org/10.1146/annurev-biodatasci-092820-114757.\n\n\nChouldechova, Alexandra. 2017. “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.” Big Data 5 (2): 153–63. https://doi.org/10.1089/big.2016.0047.\n\n\n———. 2020. “Transparency and Simplicity in Criminal Risk Assessment.” Harvard Data Science Review 2 (1). https://doi.org/10.1162/99608f92.b9343eec.\n\n\nChowdhury, Rumman. 2021. “Sharing Learnings about Our Image Cropping Algorithm.” https://blog.twitter.com/engineering/en_us/topics/insights/2021/sharing-learnings-about-our-image-cropping-algorithm.\n\n\nCrawford, Kate. 2021. “State.” In, 181–209. Power, Politics, and the Planetary Costs of Artificial Intelligence. Yale University Press. https://doi.org/10.2307/j.ctv1ghv45t.9.\n\n\nEubanks, Virginia. 2018. “A Child Abuse Prediction Model Fails Poor Families.” Wired, January. https://www.wired.com/story/excerpt-from-automating-inequality/.\n\n\nFeathers, Todd. 2021. “Police Are Telling ShotSpotter to Alter Evidence From Gunshot-Detecting AI.” https://www.vice.com/en/article/qj8xbq/police-are-telling-shotspotter-to-alter-evidence-from-gunshot-detecting-ai.\n\n\nFriedman, Batya, and Helen Nissenbaum. 1996. “Bias in Computer Systems.” ACM Transactions on Information Systems 14 (3): 330347. https://doi.org/10.1145/230538.230561.\n\n\nGilman, Michele. 2020. “Poverty Lawgorithms.” https://datasociety.net/library/poverty-lawgorithms/.\n\n\nGreen, Ben. 2019. “The Innovative City: The Relationship Between Technical and Nontechnical Change in City Government.” In The Smart Enough City: Putting Technology in Its Place to Reclaim Our Urban Future. https://direct.mit.edu/books/book/4204/chapter/172388/The-Innovative-City-The-Relationship-between.\n\n\n———. 2020. “The False Promise of Risk Assessments: Epistemic Reform and the Limits of Fairness.” In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 594–606. FAT* ’20. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3351095.3372869.\n\n\nGupta, Abhishek, and Victoria Heath. 2020. “AI Ethics Groups Are Repeating One of Society’s Classic Mistakes.” MIT Technology Review, September. https://www.technologyreview.com/2020/09/14/1008323/ai-ethics-representation-artificial-intelligence-opinion/.\n\n\nGurley, Lauren Kaori. 2021. “Amazon Drivers Are Instructed to Drive Recklessly to Meet Delivery Quotas.” https://www.vice.com/en/article/xgxx54/amazon-drivers-are-instructed-to-drive-recklessly-to-meet-delivery-quotas.\n\n\nHeilweil, Rebecca. 2020. “Facebook Is Taking a Hard Look at Racial Bias in Its Algorithms.” Vox, July. https://www.vox.com/recode/2020/7/22/21334051/facebook-news-feed-instagram-algorithm-racial-bias-civil-rights-audit.\n\n\nHenriques-Gomes, Luke. 2019. “The Automated System Leaving Welfare Recipients Cut Off with Nowhere to Turn.” The Guardian, October. http://www.theguardian.com/technology/2019/oct/16/automated-messages-welfare-australia-system.\n\n\nHill, Kashmir. 2020. “Wrongfully Accused by an Algorithm.” The New York Times, June. https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html.\n\n\nHill, Kashmir, and Aaron Krolik. 2019. “How Photos of Your Kids Are Powering Surveillance Technology.” The New York Times, October. https://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-recognition.html.\n\n\nHooker, Sara. 2021. “Moving Beyond “Algorithmic Bias Is a Data Problem”.” Patterns 2 (4). https://doi.org/10.1016/j.patter.2021.100241.\n\n\nIngold, David, and Spencer Soper. 2016. “Amazon Doesn’t Consider the Race of Its Customers. Should It?” http://www.bloomberg.com/graphics/2016-amazon-same-day/.\n\n\nJackson, Eugenie, and Christina Mendoza. 2020. “Setting the Record Straight: What the COMPAS Core Risk and Need Assessment Is and Is Not.” Harvard Data Science Review 2 (1). https://doi.org/10.1162/99608f92.1b3dadaa.\n\n\nJeffries, Adrianne, Leon Yin, and Surya Mattu. 2020. “Swinging the Vote?” The Markup, February. https://themarkup.org/google-the-giant/2020/02/26/wheres-my-email.\n\n\nJiang, Liwei. 2021. “Towards Machine Ethics and Norms.” https://medium.com/ai2-blog/towards-machine-ethics-and-norms-d64f2bdde6a3.\n\n\nKearns, Michael, and Aaron Roth. 2019. “Introduction.” In The Ethical Algorithm: The Science of Socially Aware Algorithm Design. Oxford, UNITED STATES: Oxford University Press USA - OSO. http://ebookcentral.proquest.com/lib/washington/detail.action?docID=5905172.\n\n\nKirchner, Lauren. 2020. “When Zombie Data Costs You a Home.” The Markup, October. https://themarkup.org/locked-out/2020/10/06/zombie-criminal-records-housing-background-checks.\n\n\n———. 2021. “Powerful DNA Software Used in Hundreds of Criminal Cases Faces New Scrutiny.” The Markup, March. https://themarkup.org/news/2021/03/09/powerful-dna-software-used-in-hundreds-of-criminal-cases-faces-new-scrutiny.\n\n\nLum, Kristian, and Rumman Chowdhury. 2021. “What Is an ‘Algorithm’? It Depends Whom You Ask.” MIT Technology Review, February. https://www.technologyreview.com/2021/02/26/1020007/what-is-an-algorithm/.\n\n\nLum, Kristian, and William Isaac. 2016. “To Predict and Serve?” Significance 13 (5): 14–19. https://doi.org/https://doi.org/10.1111/j.1740-9713.2016.00960.x.\n\n\nMcVean, Ada. 2019. “40 Years of Human Experimentation in America: The Tuskegee Study.” https://www.mcgill.ca/oss/article/history/40-years-human-experimentation-america-tuskegee-study.\n\n\nMervis, Jeffery. 2019. “Can a Set of Equations Keep U.S. Census Data Private?” ScienceInsider. https://www.science.org/content/article/can-set-equations-keep-us-census-data-private.\n\n\nMIT Media Lab. 2018. “Gender Shades,” February. https://www.youtube.com/watch?v=TWWsW1w-BVo.\n\n\nMIT Technology Review. 2021. “Podcast: In Machines We Trust - Hired by an Algorithm,” June. https://www.youtube.com/watch?v=ztcVB_zh_M0.\n\n\nMitchell, Shira, Eric Potash, Solon Barocas, Alexander D’Amour, and Kristian Lum. 2021. “Algorithmic Fairness: Choices, Assumptions, and Definitions.” Annual Review of Statistics and Its Application 8 (1): null. https://doi.org/10.1146/annurev-statistics-042720-125902.\n\n\nMullainathan, Sendhil. 2019. “Biased Algorithms Are Easier to Fix Than Biased People.” The New York Times, December. https://www.nytimes.com/2019/12/06/business/algorithm-bias-fix.html.\n\n\nNIckelsburg, Monica. 2021. “Washington State Lawmakers Seek to Ban Government from Using Discriminatory AI Tech.” GeekWire, February. https://www.geekwire.com/2021/washington-state-lawmakers-seek-ban-government-using-ai-tech-discriminates/.\n\n\nNoorden, Richard Van. 2020. “The Ethical Questions That Haunt Facial-Recognition Research.” Nature 587 (7834): 354–58. https://doi.org/10.1038/d41586-020-03187-3.\n\n\nO’Neill, James. 2019. “How Facial Recognition Makes You Safer.” The New York Times, June. https://www.nytimes.com/2019/06/09/opinion/facial-recognition-police-new-york-city.html.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53. https://doi.org/10.1126/science.aax2342.\n\n\nOchigame, Rodrigo. 2020. “The Long History of Algorithmic Fairness.” https://phenomenalworld.org/analysis/long-history-algorithmic-fairness.\n\n\nOnuoha, Mimi. 2020. “When Proof Is Not Enough.” https://fivethirtyeight.com/features/when-proof-is-not-enough/.\n\n\nRaghavan, Manish, and Solon Barocas. 2019. “Challenges for Mitigating Bias in Algorithmic Hiring.” https://www.brookings.edu/research/challenges-for-mitigating-bias-in-algorithmic-hiring/.\n\n\nRaji, Deborah. 2019. “That’s Not Fair!” XRDS: Crossroads, The ACM Magazine for Students 25 (3): 44–48. https://doi.org/10.1145/3313127.\n\n\nRaji, Inioluwa Deborah, and Joy Buolamwini. 2019. “Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products.” In, 429435. AIES ’19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3306618.3314244.\n\n\nRichardson, Rashida. 2019. “Confronting Black Boxes: A Shadow Report of the New York City Automated Decision System Task Force.” https://ainowinstitute.org/ads-shadowreport-2019.html.\n\n\nRudin, Cynthia, Caroline Wang, and Beau Coker. 2020. “The Age of Secrecy and Unfairness in Recidivism Prediction.” Harvard Data Science Review 2 (1). https://doi.org/10.1162/99608f92.6ed64b30.\n\n\nSmith, Ben. 2021. “How TikTok Reads Your Mind.” The New York Times, December. https://www.nytimes.com/2021/12/05/business/media/tiktok-algorithm.html.\n\n\nstabilityai. n.d. “Stable Diffusion - a Hugging Face Space.” https://huggingface.co/spaces/stabilityai/stable-diffusion.\n\n\nSudhir, K., and Shyam Sunder. 2020. “What Happens When a Billion Identities Are Digitized?” https://insights.som.yale.edu/insights/what-happens-when-billion-identities-are-digitized.\n\n\nThe Financial Times. 2020. “The Uber Game.” https://ig.ft.com/uber-game.\n\n\nUpchurch, Tom. 2018. “To Work for Society, Data Scientists Need a Hippocratic Oath with Teeth.” Wired UK. https://www.wired.co.uk/article/data-ai-ethics-hippocratic-oath-cathy-o-neil-weapons-of-math-destruction.\n\n\nVarner, Maddy, and Aaron Sankin. 2020. “Suckers List: How Allstate’s Secret Auto Insurance Algorithm Squeezes Big Spenders.” The Markup, February. https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list.\n\n\nVox. 2021. “Are We Automating Racism?” March. https://www.youtube.com/watch?v=Ok5sKLXqynQ.\n\n\nWezerek, Gus, and David Van Riper. 2020. “Opinion | Changes to the Census Could Make Small Towns Disappear.” The New York Times, February. https://www.nytimes.com/interactive/2020/02/06/opinion/census-algorithm-privacy.html.\n\n\nWhitby, Andrew. 2020. “A Brief History of the Censusand How Covid-19 Could Change It.” Wired, April. https://www.wired.com/story/brief-history-census-how-covid-19-could-change-it/.\n\n\nWolford, Ben. 2018. “What Is GDPR, the EU’s New Data Protection Law?” https://gdpr.eu/what-is-gdpr/.\n\n\nWood, Alexandra, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi, James Honaker, Kobbi Nissim, David O’Brien, Thomas Steinke, and Salil Vadhan. 2018. “Differential Privacy: A Primer for a Non-Technical Audience.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3338027."
  }
]